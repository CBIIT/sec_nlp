# -*- coding: utf-8 -*-
"""prior_therapy_classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CvuypcSfsrelOdBLDiOcVcPtL5NdZTHt
"""

# %%
!curl -L https://huggingface.co/ChrisLiewJY/BERTweet-Hedge/resolve/main/training_args.bin -O

# %%
!pip install -U transformers 'torch<2.7' evaluate datasets ray[tune] huggingface_hub fsspec==2025.3.2

# %%
import torch
from transformers import Trainer, TrainingArguments, IntervalStrategy, SchedulerType
from transformers.training_args import OptimizerNames

# Specify the path to the training_args.bin file
path_to_training_args = "training_args.bin"
torch.serialization.add_safe_globals([TrainingArguments, IntervalStrategy, SchedulerType, OptimizerNames])

# Load the training arguments using torch.load()
training_args = torch.load(path_to_training_args, weights_only=True)

# %%[markdown]
"""> I'm not sure why this didn't work. Maybe the Python version / package versions? It worked on my machine with Python 3.9, torch 2.7 and transformers 4.38"""

# %%
from datasets import Dataset, load_from_disk, ClassLabel
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split

model_name="vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Load your CSV into a DataFrame
df = pd.read_csv("pt_inclusions.csv")

# Stratified split
train_df, eval_df = train_test_split(
    df,
    test_size=0.2,
    stratify=df["label"],
    random_state=42
)

# Convert to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
eval_dataset = Dataset.from_pandas(eval_df.reset_index(drop=True))

def map_labels_to_ids(examples):
  examples['label'] = 0 if examples["label"] == "LABEL_0" else 1
  return examples

# Apply the mapping to your dataset
train_dataset = train_dataset.map(map_labels_to_ids)
eval_dataset = eval_dataset.map(map_labels_to_ids)

def tokenize(examples):
    return tokenizer(examples["text"], max_length=128, truncation=True)

train_dataset = train_dataset.map(tokenize, batched=True)
eval_dataset = eval_dataset.map(tokenize, batched=True)

train_dataset

# %%
from transformers import Trainer
from transformers import TrainingArguments, DataCollatorWithPadding
from ray import tune
import numpy as np
import evaluate

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir="pt_classifier",
    eval_strategy="epoch",
    report_to="none",
)


def model_init(trial):
    return AutoModelForSequenceClassification.from_pretrained(
        model_name,
    )


def ray_hp_space(trial):
    return {
        "learning_rate": tune.loguniform(1e-6, 1e-4),
        "per_device_train_batch_size": tune.choice([8, 16, 32, 64]),
        "num_train_epochs": tune.choice([2, 3, 9, 15]),
        "weight_decay": tune.loguniform(1e-2, 1e-1)
    }


metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # convert the logits to their predicted class
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model=None,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    processing_class=tokenizer,
    model_init=model_init,
    data_collator=data_collator,
)

best_trials = trainer.hyperparameter_search(
    direction=["min", "max"],
    backend="ray",
    hp_space=ray_hp_space,
    n_trials=1,
)

best_trials
# %%

from transformers import TrainingArguments, Trainer, DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir="pt_classifier",
    eval_strategy="epoch",
    report_to="none",
    **best_trials.hyperparameters,
)

training_args
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
    data_collator=data_collator
)

trainer.train()

# %%
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import pipeline

# The directory where your trainer saved the model
output_dir = "pt_classifier/checkpoint-3"

# Load the trained model
mymodel = AutoModelForSequenceClassification.from_pretrained(output_dir)

classifier = pipeline("text-classification", model=mymodel, tokenizer=tokenizer)
classifier

assert classifier("Prior exposure to venetoclax is allowed")[0]['label'] == 'LABEL_1'
# %%

from transformers import AutoConfig

# model_name = "allenai/scibert_scivocab_uncased"
# model_name = "kauffinger/scibert_scivocab_uncased-mnli"
# model_name = "ChrisLiewJY/BERTweet-Hedge"
model_name = "emilyalsentzer/Bio_ClinicalBERT"
AutoConfig.from_pretrained(model_name)

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-classification", model="kauffinger/scibert_scivocab_uncased-mnli")

def tune_transformer(num_samples=8, gpus_per_trial=0, smoke_test=False):
    # Change these as needed.
    model_name = "vinai/bertweet-base"

    num_labels = 2

    config = AutoConfig.from_pretrained(
        model_name, num_labels=num_labels
    )

    # Download and cache tokenizer, model, and features
    print("Downloading and caching Tokenizer")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Triggers tokenizer download to cache
    print("Downloading and caching pre-trained model")
    AutoModelForSequenceClassification.from_pretrained(
        model_name,
        config=config,
    )

    def get_model():
        return AutoModelForSequenceClassification.from_pretrained(
            model_name,
            config=config,
        )

    # Download data.
    download_data(task_name, data_dir)

    data_args = GlueDataTrainingArguments(task_name=task_name, data_dir=task_data_dir)

    train_dataset = GlueDataset(
        data_args, tokenizer=tokenizer, mode="train", cache_dir=task_data_dir
    )
    eval_dataset = GlueDataset(
        data_args, tokenizer=tokenizer, mode="dev", cache_dir=task_data_dir
    )

    training_args = TrainingArguments(
        output_dir=".",
        learning_rate=1e-5,  # config
        do_train=True,
        do_eval=True,
        no_cuda=gpus_per_trial <= 0,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        num_train_epochs=2,  # config
        max_steps=-1,
        per_device_train_batch_size=16,  # config
        per_device_eval_batch_size=16,  # config
        warmup_steps=0,
        weight_decay=0.1,  # config
        logging_dir="./logs",
        skip_memory_metrics=True,
        report_to="none",
    )

    trainer = Trainer(
        model_init=get_model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=build_compute_metrics_fn(task_name),
    )

    tune_config = {
        "per_device_train_batch_size": 32,
        "per_device_eval_batch_size": 32,
        "num_train_epochs": tune.choice([2, 3, 4, 5]),
        "max_steps": 1 if smoke_test else -1,  # Used for smoke test.
    }

    scheduler = PopulationBasedTraining(
        time_attr="training_iteration",
        metric="eval_acc",
        mode="max",
        perturbation_interval=1,
        hyperparam_mutations={
            "weight_decay": tune.uniform(0.0, 0.3),
            "learning_rate": tune.uniform(1e-5, 5e-5),
            "per_device_train_batch_size": [16, 32, 64],
        },
    )

    reporter = CLIReporter(
        parameter_columns={
            "weight_decay": "w_decay",
            "learning_rate": "lr",
            "per_device_train_batch_size": "train_bs/gpu",
            "num_train_epochs": "num_epochs",
        },
        metric_columns=["eval_acc", "eval_loss", "epoch", "training_iteration"],
    )

    trainer.hyperparameter_search(
        hp_space=lambda _: tune_config,
        backend="ray",
        n_trials=num_samples,
        resources_per_trial={"cpu": 1, "gpu": gpus_per_trial},
        scheduler=scheduler,
        keep_checkpoints_num=1,
        checkpoint_score_attr="training_iteration",
        stop={"training_iteration": 1} if smoke_test else None,
        progress_reporter=reporter,
        local_dir="~/ray_results/",
        name="tune_transformer_pbt",
        log_to_file=True,
    )

# %%
# from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from sklearn.linear_model import LogisticRegression
import pandas as pd

df = pd.read_csv('pt_inclusions.csv')
texts = df['text'].to_list()
labels = [0 if lab == 'LABEL_0' else 1 for lab in df['label']]

# model = SentenceTransformer("emilyalsentzer/Bio_ClinicalBERT")
model = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")

def get_embeddings(texts):
  return model.encode(texts, return_tensors="pt", truncation=True, padding="max_length", max_length=278)['input_ids']

embeddings = get_embeddings(texts)
# # embeddings = model.encode(texts)
print(embeddings.shape)

clf = LogisticRegression().fit(embeddings, labels)

embeddings = get_embeddings(["Prior exposure to venetoclax is required", "Patients must have had at least 1 round of treatment with venetoclax and cisplatin.", "Prior treatment with bevacizumab allowed."])

print(embeddings.shape)

clf.predict(embeddings)
